{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Trustworthy Retrieval-Augmented Generation with the Trustworthy Language Model**"
      ],
      "metadata": {
        "id": "tyrLC910OuVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SetUp**"
      ],
      "metadata": {
        "id": "KhNE_SLJO3-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary Package"
      ],
      "metadata": {
        "id": "7oTBNGA1O-vA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U cleanlab-studio llama-index llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "ISR_YPQUO65J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index llama-index-readers-web"
      ],
      "metadata": {
        "id": "gmNFGDrMQ0_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cleanlab_studio import Studio\n",
        "\n",
        "studio = Studio(\"cleanlab-studio-apikey\")"
      ],
      "metadata": {
        "id": "mnk-YlJmPoC3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Integrate TLM with LlamaIndex**"
      ],
      "metadata": {
        "id": "wzoffLYbP0Ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tlm = studio.TLM()"
      ],
      "metadata": {
        "id": "_UsQ1_tKPxFH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict\n",
        "import json\n",
        "\n",
        "# Import LlamaIndex dependencies\n",
        "from llama_index.core.base.llms.types import (\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "from llama_index.core.llms.custom import CustomLLM\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "\n",
        "class TLMWrapper(CustomLLM):\n",
        "    context_window: int = 16000\n",
        "    num_output: int = 256\n",
        "    model_name: str = \"TLM\"\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        \"\"\"Get LLM metadata.\"\"\"\n",
        "        return LLMMetadata(\n",
        "            context_window=self.context_window,\n",
        "            num_output=self.num_output,\n",
        "            model_name=self.model_name,\n",
        "        )\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
        "        # Prompt tlm for a response and trustworthiness score\n",
        "        response: Dict[str, str] = tlm.prompt(prompt)\n",
        "        output = json.dumps(response)\n",
        "        return CompletionResponse(text=output)\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
        "        # Prompt tlm for a response and trustworthiness score\n",
        "        response = tlm.prompt(prompt)\n",
        "        output = json.dumps(response)\n",
        "\n",
        "        # Stream the output\n",
        "        output_str = \"\"\n",
        "        for token in output:\n",
        "            output_str += token\n",
        "            yield CompletionResponse(text=output_str, delta=token)"
      ],
      "metadata": {
        "id": "9OSpGSq6P8Z_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build a RAG pipeline with TLM**"
      ],
      "metadata": {
        "id": "VQfZ_swIQFnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.llm = TLMWrapper()"
      ],
      "metadata": {
        "id": "CdAmufryQCAn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Specify Embedding Model**"
      ],
      "metadata": {
        "id": "J_hgnT5MQMdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "B7eilvfkQK-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Data and Create Index + Query Engine**"
      ],
      "metadata": {
        "id": "4qIjJgu_QZs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from IPython.display import Markdown, display\n",
        "import os"
      ],
      "metadata": {
        "id": "89LBOmqbQjMR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://paulgraham.com/worked.html\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Ct2RpBgEQ_ve"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "id": "QJNYEwkTRHeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "YRl541EAQUAd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "NiKrpxpJRYfz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answering queries with our RAG system**"
      ],
      "metadata": {
        "id": "mA-KNTkTRgiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This method presents formatted responses from our TLM-based RAG pipeline. It parses the output to display both the response itself and the corresponding trustworthiness score.\n",
        "def display_response(response):\n",
        "    response_str = response.response\n",
        "    output_dict = json.loads(response_str)\n",
        "    print(f\"Response: {output_dict['response']}\")\n",
        "    print(f\"Trustworthiness score: {round(output_dict['trustworthiness_score'], 2)}\")"
      ],
      "metadata": {
        "id": "K_ifKrM5RcpI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Questions**"
      ],
      "metadata": {
        "id": "BYxeyLGxRrhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"What did the author do growing up?\"\n",
        ")\n",
        "display_response(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxhlbV3aRqg5",
        "outputId": "21060248-19b6-4050-8004-2d79bb91b127"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Growing up, the author worked on writing and programming. They wrote short stories and tried programming on an IBM 1401 computer. They later got a microcomputer and started programming on it, writing simple games and a word processor.\n",
            "Trustworthiness score: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"Can I mirror an essay on my site??\"\n",
        ")\n",
        "display_response(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKIdaU3XSzif",
        "outputId": "78129588-bffa-4c3c-a333-d262ed0dd959"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Yes, you can mirror an essay on your site. The author mentions that anyone can publish anything on the web, so you have the freedom to publish and share essays online.\n",
            "Trustworthiness score: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "njE-b9uJS3-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}